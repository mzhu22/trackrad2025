{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fed166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\n",
      "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\n",
      "nnUNet_results is not defined and nnU-Net cannot be used for training or inference. If this is not intended behavior, please read documentation/setting_up_paths.md for information on how to set this up.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353d793e31ff4dc18055cdfaf85ab6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 56/56 [00:01<00:00, 34.11it/s]\n",
      "/rodata/mnradonc_dev/m299164/trackrad/trackrad-model/.venv/lib/python3.11/site-packages/sam2/sam2_video_predictor.py:786: UserWarning: cannot import name '_C' from 'sam2' (/rodata/mnradonc_dev/m299164/trackrad/trackrad-model/.venv/lib/python3.11/site-packages/sam2/__init__.py)\n",
      "\n",
      "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/sam2/blob/main/INSTALL.md).\n",
      "  pred_masks_gpu = fill_holes_in_mask_scores(\n",
      "propagate in video: 100%|██████████| 56/56 [00:01<00:00, 47.02it/s]\n",
      "frame loading (JPEG): 100%|██████████| 37/37 [00:00<00:00, 37.02it/s]\n",
      "propagate in video: 100%|██████████| 37/37 [00:00<00:00, 44.59it/s]\n",
      "frame loading (JPEG): 100%|██████████| 40/40 [00:01<00:00, 38.94it/s]\n",
      "propagate in video: 100%|██████████| 40/40 [00:00<00:00, 47.50it/s]\n",
      "frame loading (JPEG): 100%|██████████| 56/56 [00:01<00:00, 35.19it/s]\n",
      "propagate in video: 100%|██████████| 56/56 [00:01<00:00, 47.53it/s]\n",
      "frame loading (JPEG): 100%|██████████| 60/60 [00:01<00:00, 45.76it/s]\n",
      "propagate in video: 100%|██████████| 60/60 [00:01<00:00, 47.55it/s]\n",
      "frame loading (JPEG): 100%|██████████| 58/58 [00:01<00:00, 37.75it/s]\n",
      "propagate in video: 100%|██████████| 58/58 [00:01<00:00, 45.41it/s]\n",
      "frame loading (JPEG): 100%|██████████| 60/60 [00:01<00:00, 41.91it/s]\n",
      "propagate in video: 100%|██████████| 60/60 [00:01<00:00, 45.75it/s]\n",
      "frame loading (JPEG): 100%|██████████| 78/78 [00:02<00:00, 37.52it/s]\n",
      "propagate in video: 100%|██████████| 78/78 [00:01<00:00, 47.89it/s]\n",
      "frame loading (JPEG): 100%|██████████| 78/78 [00:02<00:00, 31.36it/s]\n",
      "propagate in video: 100%|██████████| 78/78 [00:01<00:00, 44.82it/s]\n",
      "frame loading (JPEG): 100%|██████████| 80/80 [00:02<00:00, 34.60it/s]\n",
      "propagate in video: 100%|██████████| 80/80 [00:01<00:00, 45.73it/s]\n",
      "frame loading (JPEG): 100%|██████████| 53/53 [00:01<00:00, 41.86it/s]\n",
      "propagate in video: 100%|██████████| 53/53 [00:01<00:00, 45.36it/s]\n",
      "frame loading (JPEG): 100%|██████████| 54/54 [00:01<00:00, 38.34it/s]\n",
      "propagate in video: 100%|██████████| 54/54 [00:01<00:00, 44.70it/s]\n",
      "frame loading (JPEG): 100%|██████████| 54/54 [00:01<00:00, 41.99it/s]\n",
      "propagate in video: 100%|██████████| 54/54 [00:01<00:00, 44.89it/s]\n",
      "frame loading (JPEG): 100%|██████████| 53/53 [00:01<00:00, 37.99it/s]\n",
      "propagate in video: 100%|██████████| 53/53 [00:01<00:00, 44.94it/s]\n",
      "frame loading (JPEG):  14%|█▍        | 9/63 [00:00<00:01, 37.56it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     56\u001b[39m labels = labels[:, :, first_nonzero_idx:]\n\u001b[32m     57\u001b[39m label_first = labels[:, :, first_nonzero_idx : first_nonzero_idx + \u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m predicted_masks = \u001b[43mrun_algorithm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mcase\u001b[39;49;00m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m frames.shape == predicted_masks.shape == labels.shape\n\u001b[32m     73\u001b[39m (OUTPUT / \u001b[33m\"\u001b[39m\u001b[33mimagesTr\u001b[39m\u001b[33m\"\u001b[39m).mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/rodata/mnradonc_dev/m299164/trackrad/trackrad-model/model.py:134\u001b[39m, in \u001b[36mrun_algorithm\u001b[39m\u001b[34m(case_id, predictor, refiner, frames, target, frame_rate, magnetic_field_strength, scanned_region, do_refinement)\u001b[39m\n\u001b[32m    131\u001b[39m path = Path(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./tmp/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcase_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    132\u001b[39m jpeg_path = save_mri_series_as_jpegs(path, frames)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m inference_state = \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjpeg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m first_frame_mask = target[:, :, \u001b[32m0\u001b[39m]\n\u001b[32m    136\u001b[39m obj_id = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/rodata/mnradonc_dev/m299164/trackrad/trackrad-model/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/rodata/mnradonc_dev/m299164/trackrad/trackrad-model/.venv/lib/python3.11/site-packages/sam2/sam2_video_predictor.py:51\u001b[39m, in \u001b[36mSAM2VideoPredictor.init_state\u001b[39m\u001b[34m(self, video_path, offload_video_to_cpu, offload_state_to_cpu, async_loading_frames)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Initialize an inference state.\"\"\"\u001b[39;00m\n\u001b[32m     50\u001b[39m compute_device = \u001b[38;5;28mself\u001b[39m.device  \u001b[38;5;66;03m# device of the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m images, video_height, video_width = \u001b[43mload_video_frames\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffload_video_to_cpu\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_video_to_cpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43masync_loading_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43masync_loading_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m inference_state = {}\n\u001b[32m     59\u001b[39m inference_state[\u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m] = images\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/rodata/mnradonc_dev/m299164/trackrad/trackrad-model/.venv/lib/python3.11/site-packages/sam2/utils/misc.py:198\u001b[39m, in \u001b[36mload_video_frames\u001b[39m\u001b[34m(video_path, image_size, offload_video_to_cpu, img_mean, img_std, async_loading_frames, compute_device)\u001b[39m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m load_video_frames_from_video_file(\n\u001b[32m    190\u001b[39m         video_path=video_path,\n\u001b[32m    191\u001b[39m         image_size=image_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    195\u001b[39m         compute_device=compute_device,\n\u001b[32m    196\u001b[39m     )\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_str \u001b[38;5;129;01mand\u001b[39;00m os.path.isdir(video_path):\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_video_frames_from_jpg_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_video_to_cpu\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_video_to_cpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimg_mean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimg_std\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43masync_loading_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43masync_loading_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    209\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOnly MP4 video and JPEG folder are supported at this moment\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    210\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/rodata/mnradonc_dev/m299164/trackrad/trackrad-model/.venv/lib/python3.11/site-packages/sam2/utils/misc.py:269\u001b[39m, in \u001b[36mload_video_frames_from_jpg_images\u001b[39m\u001b[34m(video_path, image_size, offload_video_to_cpu, img_mean, img_std, async_loading_frames, compute_device)\u001b[39m\n\u001b[32m    267\u001b[39m images = torch.zeros(num_frames, \u001b[32m3\u001b[39m, image_size, image_size, dtype=torch.float32)\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n, img_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(img_paths, desc=\u001b[33m\"\u001b[39m\u001b[33mframe loading (JPEG)\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     images[n], video_height, video_width = \u001b[43m_load_img_as_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m offload_video_to_cpu:\n\u001b[32m    271\u001b[39m     images = images.to(compute_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/rodata/mnradonc_dev/m299164/trackrad/trackrad-model/.venv/lib/python3.11/site-packages/sam2/utils/misc.py:99\u001b[39m, in \u001b[36m_load_img_as_tensor\u001b[39m\u001b[34m(img_path, image_size)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown image dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_np.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m img = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_np\u001b[49m\u001b[43m)\u001b[49m.permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    100\u001b[39m video_width, video_height = img_pil.size  \u001b[38;5;66;03m# the original video size\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img, video_height, video_width\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from ..inference import setup_sam2\n",
    "from ..model import run_algorithm\n",
    "\n",
    "DATASET = Path(\n",
    "    \"/rodata/mnradonc_dev/m299164/trackrad/datasets/duke-liver-v2/Segmentation\"\n",
    ")\n",
    "CHECKPOINT = Path(\"./resources/sam2.1_hiera_small_trackrad_07_21.pt\")\n",
    "OUTPUT = Path(\n",
    "    \"/rodata/mnradonc_dev/m299164/trackrad/datasets/nnUNet/nnUNet_raw/Dataset400_DukeLiver\"\n",
    ")\n",
    "DAVIS_PALETTE = b\"\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x80\\x80\\x00\\x00\\x00\\x80\\x80\\x00\\x80\\x00\\x80\\x80\\x80\\x80\\x80@\\x00\\x00\\xc0\\x00\\x00@\\x80\\x00\\xc0\\x80\\x00@\\x00\\x80\\xc0\\x00\\x80@\\x80\\x80\\xc0\\x80\\x80\\x00@\\x00\\x80@\\x00\\x00\\xc0\\x00\\x80\\xc0\\x00\\x00@\\x80\\x80@\\x80\\x00\\xc0\\x80\\x80\\xc0\\x80@@\\x00\\xc0@\\x00@\\xc0\\x00\\xc0\\xc0\\x00@@\\x80\\xc0@\\x80@\\xc0\\x80\\xc0\\xc0\\x80\\x00\\x00@\\x80\\x00@\\x00\\x80@\\x80\\x80@\\x00\\x00\\xc0\\x80\\x00\\xc0\\x00\\x80\\xc0\\x80\\x80\\xc0@\\x00@\\xc0\\x00@@\\x80@\\xc0\\x80@@\\x00\\xc0\\xc0\\x00\\xc0@\\x80\\xc0\\xc0\\x80\\xc0\\x00@@\\x80@@\\x00\\xc0@\\x80\\xc0@\\x00@\\xc0\\x80@\\xc0\\x00\\xc0\\xc0\\x80\\xc0\\xc0@@@\\xc0@@@\\xc0@\\xc0\\xc0@@@\\xc0\\xc0@\\xc0@\\xc0\\xc0\\xc0\\xc0\\xc0 \\x00\\x00\\xa0\\x00\\x00 \\x80\\x00\\xa0\\x80\\x00 \\x00\\x80\\xa0\\x00\\x80 \\x80\\x80\\xa0\\x80\\x80`\\x00\\x00\\xe0\\x00\\x00`\\x80\\x00\\xe0\\x80\\x00`\\x00\\x80\\xe0\\x00\\x80`\\x80\\x80\\xe0\\x80\\x80 @\\x00\\xa0@\\x00 \\xc0\\x00\\xa0\\xc0\\x00 @\\x80\\xa0@\\x80 \\xc0\\x80\\xa0\\xc0\\x80`@\\x00\\xe0@\\x00`\\xc0\\x00\\xe0\\xc0\\x00`@\\x80\\xe0@\\x80`\\xc0\\x80\\xe0\\xc0\\x80 \\x00@\\xa0\\x00@ \\x80@\\xa0\\x80@ \\x00\\xc0\\xa0\\x00\\xc0 \\x80\\xc0\\xa0\\x80\\xc0`\\x00@\\xe0\\x00@`\\x80@\\xe0\\x80@`\\x00\\xc0\\xe0\\x00\\xc0`\\x80\\xc0\\xe0\\x80\\xc0 @@\\xa0@@ \\xc0@\\xa0\\xc0@ @\\xc0\\xa0@\\xc0 \\xc0\\xc0\\xa0\\xc0\\xc0`@@\\xe0@@`\\xc0@\\xe0\\xc0@`@\\xc0\\xe0@\\xc0`\\xc0\\xc0\\xe0\\xc0\\xc0\\x00 \\x00\\x80 \\x00\\x00\\xa0\\x00\\x80\\xa0\\x00\\x00 \\x80\\x80 \\x80\\x00\\xa0\\x80\\x80\\xa0\\x80@ \\x00\\xc0 \\x00@\\xa0\\x00\\xc0\\xa0\\x00@ \\x80\\xc0 \\x80@\\xa0\\x80\\xc0\\xa0\\x80\\x00`\\x00\\x80`\\x00\\x00\\xe0\\x00\\x80\\xe0\\x00\\x00`\\x80\\x80`\\x80\\x00\\xe0\\x80\\x80\\xe0\\x80@`\\x00\\xc0`\\x00@\\xe0\\x00\\xc0\\xe0\\x00@`\\x80\\xc0`\\x80@\\xe0\\x80\\xc0\\xe0\\x80\\x00 @\\x80 @\\x00\\xa0@\\x80\\xa0@\\x00 \\xc0\\x80 \\xc0\\x00\\xa0\\xc0\\x80\\xa0\\xc0@ @\\xc0 @@\\xa0@\\xc0\\xa0@@ \\xc0\\xc0 \\xc0@\\xa0\\xc0\\xc0\\xa0\\xc0\\x00`@\\x80`@\\x00\\xe0@\\x80\\xe0@\\x00`\\xc0\\x80`\\xc0\\x00\\xe0\\xc0\\x80\\xe0\\xc0@`@\\xc0`@@\\xe0@\\xc0\\xe0@@`\\xc0\\xc0`\\xc0@\\xe0\\xc0\\xc0\\xe0\\xc0  \\x00\\xa0 \\x00 \\xa0\\x00\\xa0\\xa0\\x00  \\x80\\xa0 \\x80 \\xa0\\x80\\xa0\\xa0\\x80` \\x00\\xe0 \\x00`\\xa0\\x00\\xe0\\xa0\\x00` \\x80\\xe0 \\x80`\\xa0\\x80\\xe0\\xa0\\x80 `\\x00\\xa0`\\x00 \\xe0\\x00\\xa0\\xe0\\x00 `\\x80\\xa0`\\x80 \\xe0\\x80\\xa0\\xe0\\x80``\\x00\\xe0`\\x00`\\xe0\\x00\\xe0\\xe0\\x00``\\x80\\xe0`\\x80`\\xe0\\x80\\xe0\\xe0\\x80  @\\xa0 @ \\xa0@\\xa0\\xa0@  \\xc0\\xa0 \\xc0 \\xa0\\xc0\\xa0\\xa0\\xc0` @\\xe0 @`\\xa0@\\xe0\\xa0@` \\xc0\\xe0 \\xc0`\\xa0\\xc0\\xe0\\xa0\\xc0 `@\\xa0`@ \\xe0@\\xa0\\xe0@ `\\xc0\\xa0`\\xc0 \\xe0\\xc0\\xa0\\xe0\\xc0``@\\xe0`@`\\xe0@\\xe0\\xe0@``\\xc0\\xe0`\\xc0`\\xe0\\xc0\\xe0\\xe0\\xc0\"\n",
    "\n",
    "predictor = setup_sam2(CHECKPOINT)\n",
    "\n",
    "count = 0\n",
    "for subject in tqdm(list(DATASET.iterdir())):\n",
    "    for case in subject.iterdir():\n",
    "        count += 1\n",
    "\n",
    "        images = sorted((case / \"images\").glob(\"*.dicom\"))\n",
    "        masks = sorted((case / \"masks\").glob(\"*.dicom\"))\n",
    "\n",
    "        frames = np.stack(\n",
    "            [\n",
    "                sitk.GetArrayFromImage(sitk.ReadImage(str(img))).squeeze()\n",
    "                for img in images\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        labels = np.stack(\n",
    "            [\n",
    "                sitk.GetArrayFromImage(sitk.ReadImage(str(mask))).squeeze()\n",
    "                for mask in masks\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "\n",
    "        # Find the first frame in labels that is not all zeroes\n",
    "        first_nonzero_idx = next(\n",
    "            (i for i in range(labels.shape[-1]) if np.any(labels[:, :, i] != 0)), None\n",
    "        )\n",
    "        if first_nonzero_idx is None:\n",
    "            print(\"All label frames are zero.\")\n",
    "            continue\n",
    "\n",
    "        # Skip some frames bc the liver doesn't appear in the first slice\n",
    "        frames = frames[:, :, first_nonzero_idx:]\n",
    "        labels = labels[:, :, first_nonzero_idx:]\n",
    "        label_first = labels[:, :, first_nonzero_idx : first_nonzero_idx + 1]\n",
    "\n",
    "        predicted_masks = run_algorithm(\n",
    "            case_id=case.name,\n",
    "            predictor=predictor,\n",
    "            refiner=None,\n",
    "            frames=frames,\n",
    "            target=label_first,\n",
    "            frame_rate=1.0,\n",
    "            magnetic_field_strength=1.0,\n",
    "            scanned_region=\"test\",\n",
    "            do_refinement=False,\n",
    "        )\n",
    "\n",
    "        assert frames.shape == predicted_masks.shape == labels.shape\n",
    "\n",
    "        (OUTPUT / \"imagesTr\").mkdir(parents=True, exist_ok=True)\n",
    "        (OUTPUT / \"labelsTr\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for i in range(frames.shape[-1]):\n",
    "            fname = OUTPUT / \"imagesTr\" / f\"{subject.name}-{case.name}-{i:04d}_0000.png\"\n",
    "            frame = frames[:, :, i]\n",
    "            Image.fromarray(frame).save(fname)\n",
    "\n",
    "        for i in range(predicted_masks.shape[-1]):\n",
    "            fname = OUTPUT / \"imagesTr\" / f\"{subject.name}-{case.name}-{i:04d}_0001.png\"\n",
    "            mask = predicted_masks[:, :, i].astype(np.uint8)\n",
    "            png = Image.fromarray(mask)\n",
    "            png.putpalette(DAVIS_PALETTE)\n",
    "            png.save(fname)\n",
    "\n",
    "        for i in range(labels.shape[-1]):\n",
    "            fname = OUTPUT / \"labelsTr\" / f\"{subject.name}-{case.name}-{i:04d}.png\"\n",
    "            label = labels[:, :, i].astype(np.uint8)\n",
    "            png = Image.fromarray(label)\n",
    "            png.putpalette(DAVIS_PALETTE)\n",
    "            png.save(fname)\n",
    "\n",
    "metadata = {\n",
    "    \"channel_names\": {\n",
    "        \"0\": \"raw_image\",\n",
    "        \"1\": \"suggested_mask\",\n",
    "    },\n",
    "    \"labels\": {\n",
    "        \"background\": 0,\n",
    "        \"lesion\": 1,\n",
    "    },\n",
    "    \"numTraining\": count,\n",
    "    \"file_ending\": \".png\",\n",
    "}\n",
    "dataset_json = OUTPUT / \"dataset.json\"\n",
    "dataset_json.write_text(json.dumps(metadata, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trackrad-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
